{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db15ff76-857d-4bed-96a9-9668b6623943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.models import ResNet34_Weights\n",
    "from pandas import read_csv\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "871668e5-e6d5-459c-a037-988955810624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = read_csv('data/test.tsv', sep='\\t')\n",
    "#train_data = read_csv('data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64e6d7-5b8b-4c44-ba15-aca3a12de3f7",
   "metadata": {},
   "source": [
    "# Задание параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33cea2ab-dd6b-4df6-b173-35eb9159f6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config_json = {\n",
    "    \"alphabet\": \"абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ0123456789.,;:!?~-–+=()[]{}‘/\\\\|+-*÷=><%^,«»„“\",\n",
    "    \"save_dir\": \"data/experiment/models\",\n",
    "    \"num_epoch\": 200,\n",
    "    \"image\": {\n",
    "        \"width\": 64,\n",
    "        \"height\": 32\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"root_path\": \"data/train/\",\n",
    "        \"tsv_path\": \"data/train.tsv\",\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"root_path\": \"data/test/\",\n",
    "        \"tsv_path\": \"data/test.tsv\",\n",
    "        \"batch_size\": 128\n",
    "    }\n",
    "}\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18af0e0-72b5-48ee-8b44-0e9edd70eb03",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbcc588f-d4bb-4f26-9c79-0cc39261196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accuracy(s):\n",
    "    return(float(s[-11:-5]))\n",
    "\n",
    "def latest(path):\n",
    "    try:\n",
    "        files = os.listdir(path)\n",
    "        paths = [os.path.join(path, basename) for basename in files]\n",
    "        return max(paths, key=strip_accuracy)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, texts, enc_texts = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    text_lens = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            text_len = len(text)\n",
    "        except TypeError:\n",
    "            text_len = 6 #средняя длина слова в русском языке\n",
    "        text_lens += [text_len]\n",
    "    text_lens = torch.LongTensor(text_lens)\n",
    "    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n",
    "    return images, texts, enc_pad_texts, text_lens\n",
    "\n",
    "def get_data_loader(\n",
    "    transforms, tsv_path, root_path, tokenizer, batch_size, drop_last):\n",
    "    dataset = OCRDataset(tsv_path, root_path, tokenizer, transforms)\n",
    "    torch.set_num_threads(1)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675afe1d-0c12-4748-a33e-b7e5e7742340",
   "metadata": {},
   "source": [
    "# Определение класса датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5eb37bf-5728-43fb-9f4d-e9a3d6016b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, tsv_path, root_path, tokenizer, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        data = read_csv(tsv_path, sep='\\t')\n",
    "        self.data_len = len(data)\n",
    "\n",
    "        self.img_paths = []\n",
    "        self.texts = []\n",
    "        for index, row in data.iterrows():\n",
    "            img_name = row['Название']\n",
    "            text = row['Текст']\n",
    "            self.img_paths.append(os.path.join(root_path, img_name))\n",
    "            self.texts.append(text)\n",
    "        self.enc_texts = tokenizer.encode(self.texts)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        text = self.texts[idx]\n",
    "        enc_text = torch.LongTensor(self.enc_texts[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, text, enc_text\n",
    "        lllll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3a104-940a-43ed-bb32-1fafffed5d60",
   "metadata": {},
   "source": [
    "# Определение токенайзера - вспомогательного класса, который преобразует текст в числа\n",
    "Разметка-текст с картинок преобразуется в числовые данные, на которых производится обучение модели. Также может делать и обратное преобразование. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8dfc5b3-6d8f-4c82-9950-f8dee2dc7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>' #\"Символ\" для символов вне заданного набора\n",
    "CTC_BLANK = '<BLANK>' # \"Символ\" для пробелов\n",
    "\n",
    "def get_char_map(alphabet):\n",
    "    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n",
    "    char_map[CTC_BLANK] = 0\n",
    "    char_map[OOV_TOKEN] = 1\n",
    "    return char_map\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, alphabet):\n",
    "        self.char_map = get_char_map(alphabet)\n",
    "        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n",
    "\n",
    "    def encode(self, word_list):\n",
    "        enc_words = []\n",
    "        for word in word_list:\n",
    "            enc_words.append(\n",
    "                [self.char_map[char] if char in self.char_map\n",
    "                 else self.char_map[OOV_TOKEN] for char in str(word)])\n",
    "        return enc_words\n",
    "\n",
    "    def get_num_chars(self):\n",
    "        return len(self.char_map)\n",
    "\n",
    "    def decode(self, enc_word_list):\n",
    "        dec_words = []\n",
    "        for word in enc_word_list:\n",
    "            word_chars = ''\n",
    "            for idx, char_enc in enumerate(word):\n",
    "                if (char_enc != self.char_map[OOV_TOKEN]\n",
    "                    and char_enc != self.char_map[CTC_BLANK]\n",
    "                    and not (idx > 0 and char_enc == word[idx - 1])):\n",
    "                    word_chars += self.rev_char_map[char_enc]\n",
    "            dec_words.append(word_chars)\n",
    "        return dec_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba841e-4ce7-4aed-9411-8138f1118cb7",
   "metadata": {},
   "source": [
    "# Задание функции метрики\n",
    "В качестве метрики используется accuracy. Она измеряет долю предсказанных строк текста, которые полностью совпадают с истинным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd1b4848-f8ba-4f7a-a36a-125909f35c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        scores.append(true == pred)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2da260-492e-49a6-b649-7ecb6272de6f",
   "metadata": {},
   "source": [
    "# Аугментации модели\n",
    "Функции для обеспечения единообразия данных для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40b29fbf-b077-4437-995b-c9f08e00cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __call__(self, img):\n",
    "        img = img.astype(np.float32) / 255\n",
    "        return img\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, arr):\n",
    "        arr = torch.from_numpy(arr)\n",
    "        return arr\n",
    "\n",
    "class MoveChannels:\n",
    "    def __init__(self, to_channels_first=True):\n",
    "        self.to_channels_first = to_channels_first\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if self.to_channels_first:\n",
    "            return np.moveaxis(image, -1, 0)\n",
    "        else:\n",
    "            return np.moveaxis(image, 0, -1)\n",
    "\n",
    "class ImageResize:\n",
    "    def __init__(self, height, width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def __call__(self, image):\n",
    "        #print(image)\n",
    "        image = cv2.resize(image, (self.width, self.height),\n",
    "                            interpolation=cv2.INTER_LINEAR)\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_train_transforms(height, width):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        ImageResize(height, width),\n",
    "        MoveChannels(to_channels_first=True),\n",
    "        Normalize(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    return transforms\n",
    "\n",
    "def get_val_transforms(height, width):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        ImageResize(height, width),\n",
    "        MoveChannels(to_channels_first=True),\n",
    "        Normalize(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd224af-4a97-44cb-9d1e-b8558ea0c79d",
   "metadata": {},
   "source": [
    "# Определение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "709b2934-a649-4c90-a3a8-0e67e12c98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet34_backbone(weights=ResNet34_Weights.IMAGENET1K_V1):\n",
    "    m = torchvision.models.resnet34(weights=weights)\n",
    "    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n",
    "    blocks = [input_conv, m.bn1, m.relu, \n",
    "              m.maxpool, m.layer1, m.layer2, m.layer3]\n",
    "    return nn.Sequential(*blocks)\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            dropout=dropout, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return out\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, number_class_symbols, time_feature_count=256,\n",
    "        lstm_hidden=256, lstm_len=2):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = get_resnet34_backbone(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((time_feature_count, time_feature_count))\n",
    "        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden * 2, time_feature_count),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(time_feature_count, number_class_symbols))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.view(b, c * h, w)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.classifier(x)\n",
    "        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44c389-b0b1-47ea-95a2-fda534362cea",
   "metadata": {},
   "source": [
    "# Задание обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd796235-5146-4b4a-9926-5d7e16f7eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(data_loader, model, tokenizer, device):\n",
    "    acc_avg = AverageMeter()\n",
    "    for images, texts, _, _ in data_loader:\n",
    "        #print(images)\n",
    "        #start = time.time()\n",
    "        batch_size = len(texts)\n",
    "        #print(\"val\", batch_size, end=' ')\n",
    "        text_preds = predict(images, model, tokenizer, device)\n",
    "        acc_avg.update(get_accuracy(texts, text_preds), batch_size)\n",
    "        #end = time.time()\n",
    "        #print(end - start)\n",
    "    print(f'Validation, acc: {acc_avg.avg:.4f}')\n",
    "    return acc_avg.avg\n",
    "\n",
    "def train_loop(data_loader, model, criterion, optimizer, epoch):\n",
    "    start_cycle = time.time()\n",
    "    loss_avg = AverageMeter()\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for images, texts, enc_pad_texts, text_lens in data_loader:\n",
    "        model.zero_grad()\n",
    "        images = images.to(DEVICE)\n",
    "        batch_size = len(texts)\n",
    "        output = model(images)\n",
    "        output_lengths = torch.full(\n",
    "            size=(output.size(1),),\n",
    "            fill_value=output.size(0),\n",
    "            dtype=torch.long)\n",
    "        loss = criterion(output, enc_pad_texts, output_lengths, text_lens)\n",
    "        loss_avg.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "        optimizer.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    end_cycle = time.time()\n",
    "    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}, Time elapsed: {end_cycle - start_cycle}')\n",
    "    return loss_avg.avg\n",
    "\n",
    "def predict(images, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n",
    "    text_preds = tokenizer.decode(pred)\n",
    "    return text_preds\n",
    "\n",
    "def get_loaders(tokenizer, config):\n",
    "    train_transforms = get_train_transforms(\n",
    "        height=config['image']['height'],\n",
    "        width=config['image']['width'])\n",
    "    train_loader = get_data_loader(\n",
    "        tsv_path=config['train']['tsv_path'],\n",
    "        root_path=config['train']['root_path'],\n",
    "        transforms=train_transforms,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=config['train']['batch_size'],\n",
    "        drop_last=True)\n",
    "    val_transforms = get_val_transforms(\n",
    "        height=config['image']['height'],\n",
    "        width=config['image']['width'])\n",
    "    val_loader = get_data_loader(\n",
    "        tsv_path=config['test']['tsv_path'],\n",
    "        root_path=config['test']['root_path'],\n",
    "        transforms=val_transforms,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=config['test']['batch_size'],\n",
    "        drop_last=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train(config, checkpoint=None):\n",
    "    tokenizer = Tokenizer(config['alphabet'])\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "    train_loader, val_loader = get_loaders(tokenizer, config)\n",
    "    \n",
    "    model = CRNN(number_class_symbols=tokenizer.get_num_chars())\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    if checkpoint == None:\n",
    "        epoch = -1\n",
    "        best_acc = -np.inf    \n",
    "    else:\n",
    "        if checkpoint == 'latest':\n",
    "            checkpoint = latest('data\\\\experiment\\\\models\\\\')\n",
    "        epoch = int(checkpoint[-14:-12])\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model.eval()\n",
    "        best_acc = val_loop(val_loader, model, tokenizer, DEVICE)\n",
    "    \n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=optimizer, mode='max', factor=0.25, patience=10)\n",
    "    \n",
    "    for epoch in range(epoch + 1, config['num_epoch']):\n",
    "        loss_avg = train_loop(train_loader, model, criterion, optimizer, epoch)\n",
    "        acc_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n",
    "        scheduler.step(acc_avg)\n",
    "        if acc_avg > best_acc:\n",
    "            best_acc = acc_avg\n",
    "            model_save_path = os.path.join(\n",
    "                config['save_dir'], f'model-{epoch}-{acc_avg:.4f}.ckpt')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print('Model weights saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f213fe8-ef7e-4d63-8da3-f1addcd75fe6",
   "metadata": {},
   "source": [
    "# Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ae888-ebd7-447a-811c-8d7613a7c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(config_json, 'latest') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248da132-0173-4432-9b42-53c5ff7d21be",
   "metadata": {},
   "source": [
    "# Создание предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061ebd7-f449-4226-8761-ee223166ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceTransform:\n",
    "    def __init__(self, height, width):\n",
    "        self.transforms = get_val_transforms(height, width)\n",
    "    def __call__(self, images):\n",
    "        transformed_images = []\n",
    "        for image in images:\n",
    "            image = self.transforms(image)\n",
    "            transformed_images.append(image)\n",
    "        transformed_tensor = torch.stack(transformed_images, 0)\n",
    "        return transformed_tensor\n",
    "\n",
    "class OcrPredictor:\n",
    "    def __init__(self, model_path, config, device='cuda'):\n",
    "        self.tokenizer = Tokenizer(config['alphabet'])\n",
    "        self.device = torch.device(device)\n",
    "        # load model\n",
    "        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.model.load_state_dict(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.transforms = InferenceTransform(\n",
    "            height=config['image']['height'],\n",
    "            width=config['image']['width'],)\n",
    "\n",
    "    def __call__(self, images):\n",
    "        if isinstance(images, (list, tuple)):\n",
    "            one_image = False\n",
    "        elif isinstance(images, np.ndarray):\n",
    "            images = [images]\n",
    "            one_image = True\n",
    "        else:\n",
    "            raise Exception(f\"Input must contain np.ndarray, \"\n",
    "                            f\"tuple or list, found {type(images)}.\")\n",
    "        images = self.transforms(images)\n",
    "        pred = predict(images, self.model, self.tokenizer, self.device)\n",
    "\n",
    "        if one_image:\n",
    "            return pred[0]\n",
    "        else:\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5a613-7f40-41b0-b400-1a99ffc1d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = OcrPredictor(\n",
    "    model_path=latest('data\\\\experiment\\\\models\\\\'),\n",
    "    config=config_json\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce61a22-6498-4de6-a379-23b8b7f52f35",
   "metadata": {},
   "source": [
    "В качестве небольшой демонстрации работы, проверка работы модели на моем почерке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a6548-1fb1-4e9b-b1f6-8d14a6bf4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "print_images = True\n",
    "dir_path = 'data/experiment/test'\n",
    "for img_name in os.listdir(dir_path):\n",
    "    img = cv2.imread(f'{dir_path}/{img_name}')\n",
    "\n",
    "    pred = predictor(img)\n",
    "\n",
    "    if print_images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print('File: ', img_name)\n",
    "        print('Prediction: ', predictor(img))\n",
    "        count += 1\n",
    "\n",
    "    if count > 20:\n",
    "        break\n",
    "        print_images = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
